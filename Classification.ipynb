{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Import necessary library and read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "\n",
    "# Read in track metadata with genre labels\n",
    "tracks = pd.read_csv('datasets/fma-rock-vs-hiphop.csv')\n",
    "\n",
    "# Read in track metrics with the features\n",
    "echonest_metrics = pd.read_json('datasets/echonest-metrics.json', precise_float=True)\n",
    "\n",
    "# Merge the relevant columns of tracks and echonest_metrics\n",
    "echo_tracks = echonest_metrics.merge(tracks[['genre_top', 'track_id']], on='track_id')\n",
    "\n",
    "# Create features\n",
    "features = echo_tracks.drop([\"genre_top\", \"track_id\"], axis=1).values\n",
    "\n",
    "# Create labels\n",
    "labels = echo_tracks[\"genre_top\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition the data into traning and testing set.\n",
    "# Apply normalization to scale the data\n",
    "# Apply PCA to reduce the dimension of the data. (the optimal number of PCA components is obtained from EDA., see EDA and Data Preprocessing for detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, \n",
    "                                                                            random_state=10)\n",
    "# Scale train_features and set the values to a new variable\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale train_features and test_features\n",
    "scaled_train_features = scaler.fit_transform(train_features)\n",
    "scaled_test_features = scaler.transform(test_features)\n",
    "\n",
    "# Perform PCA with the chosen number of components and project data onto components\n",
    "pca = PCA(n_components=6, random_state=10)\n",
    "\n",
    "# Fit and transform the scaled training features using pca\n",
    "train_pca = pca.fit_transform(scaled_train_features)\n",
    "\n",
    "# Fit and transform the scaled test features using pca\n",
    "test_pca = pca.transform(scaled_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Several variants of SVM classifier can be found in train_SVM_Variants.ipynb. To use functions in that file, first import it.\n",
    "# Here we called CV_Tuned_SVM classifier in which the hyperparameters are tuned using cross validation and graidsearch method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned SVM Parameters: {'C': 1, 'gamma': 0.1}\n",
      "Best score is 0.9141917090460779\n",
      "Best Estimator is SVC(C=1, gamma=0.1)\n"
     ]
    }
   ],
   "source": [
    "#import train_SVM_Variants to call train function\n",
    "#training SVM with various options can be found in train_SVM_Variants\n",
    "from ipynb.fs.full.train_SVM_Variants import *\n",
    "\n",
    "CV_Tuned_SVM=CV_Tuned_SVM(train_pca,train_labels)\n",
    "\n",
    "print(\"Tuned SVM Parameters: {}\".format(CV_Tuned_SVM.best_params_)) \n",
    "print(\"Best score is {}\".format(CV_Tuned_SVM.best_score_))\n",
    "print(\"Best Estimator is {}\".format(CV_Tuned_SVM.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Several variants of Decision Tree classifier can be found in train_DecisionTree_Variants.ipynb. To use functions in that file, first import it.\n",
    "# Here we called CV_Tuned_DTclassifier in which the hyperparameters are tuned using cross validation and graidsearch method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 4}\n",
      "Best score is 0.8847565110186469\n",
      "Best Estimator is DecisionTreeClassifier(criterion='entropy', max_depth=4)\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_DecisionTree_Variants import *\n",
    "\n",
    "CV_Tuned_DT=CV_Tuned_DT(train_pca,train_labels)\n",
    "\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(CV_Tuned_DT.best_params_)) \n",
    "print(\"Best score is {}\".format(CV_Tuned_DT.best_score_))\n",
    "print(\"Best Estimator is {}\".format(CV_Tuned_DT.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is fine tuned LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned LogR Parameters: {'C': 0.4393970560760795}\n",
      "Best score is 0.8769814301124981\n",
      "Best Estimator is LogisticRegression(C=0.4393970560760795)\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_LogisticRegression_Variants import *\n",
    "CV_Tune_LogR=CV_Tune_LogR(train_pca,train_labels)\n",
    "\n",
    "print(\"Tuned LogR Parameters: {}\".format(CV_Tune_LogR.best_params_)) \n",
    "print(\"Best score is {}\".format(CV_Tune_LogR.best_score_))\n",
    "print(\"Best Estimator is {}\".format(CV_Tune_LogR.best_estimator_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned KNN Parameters: {'n_neighbors': 9}\n",
      "Best score is 0.9114147018030513\n",
      "Best Estimator is KNeighborsClassifier(n_neighbors=9)\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_KNN_Variants import *\n",
    "\n",
    "CV_KNN=CV_KNN(train_pca,train_labels)\n",
    "\n",
    "print(\"Tuned KNN Parameters: {}\".format(CV_KNN.best_params_)) \n",
    "print(\"Best score is {}\".format(CV_KNN.best_score_))\n",
    "print(\"Best Estimator is {}\".format(CV_KNN.best_estimator_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Random Forest Parameters: {'max_features': 'sqrt', 'min_samples_leaf': 2, 'n_estimators': 350}\n",
      "Best score is 0.9075277392510401\n",
      "Best Estimator is RandomForestClassifier(max_features='sqrt', min_samples_leaf=2,\n",
      "                       n_estimators=350)\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.train_ensemble_Variants import *\n",
    "\n",
    "CV_RF=CV_RandomForest(train_pca,train_labels)\n",
    "\n",
    "print(\"Tuned Random Forest Parameters: {}\".format(CV_RF.best_params_)) \n",
    "print(\"Best score is {}\".format(CV_RF.best_score_))\n",
    "print(\"Best Estimator is {}\".format(CV_RF.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compare the cross validation results of each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy on Traning Data\n",
      "               Accuracy\n",
      "LogR           0.876981\n",
      "Decision Tree  0.884757\n",
      "SVM            0.914192\n",
      "KNN            0.911415\n",
      "Random Forest  0.907528\n"
     ]
    }
   ],
   "source": [
    "# Compare the accuracy of CV results on train data\n",
    "\n",
    "accuracy=[CV_Tune_LogR.best_score_,CV_Tuned_DT.best_score_,CV_Tuned_SVM.best_score_,CV_KNN.best_score_,CV_RF.best_score_]\n",
    "classifiers=['LogR','Decision Tree','SVM','KNN','Random Forest']\n",
    "print('Cross Validation Accuracy on Traning Data')\n",
    "print(pd.DataFrame(accuracy,index=classifiers,columns=['Accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best parameters found during hyperparatmeters with cross validation, we will test the trained classifiers on the new data and measure the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Hip-Hop       0.77      0.54      0.64       235\n",
      "        Rock       0.90      0.96      0.93       966\n",
      "\n",
      "    accuracy                           0.88      1201\n",
      "   macro avg       0.83      0.75      0.78      1201\n",
      "weighted avg       0.87      0.88      0.87      1201\n",
      "\n",
      "\n",
      " Decision Tree: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Hip-Hop       0.75      0.61      0.67       235\n",
      "        Rock       0.91      0.95      0.93       966\n",
      "\n",
      "    accuracy                           0.88      1201\n",
      "   macro avg       0.83      0.78      0.80      1201\n",
      "weighted avg       0.88      0.88      0.88      1201\n",
      "\n",
      "\n",
      " Support Vector Machine: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Hip-Hop       0.85      0.60      0.71       235\n",
      "        Rock       0.91      0.97      0.94       966\n",
      "\n",
      "    accuracy                           0.90      1201\n",
      "   macro avg       0.88      0.79      0.82      1201\n",
      "weighted avg       0.90      0.90      0.90      1201\n",
      "\n",
      "\n",
      " K Nearest Neighbors: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Hip-Hop       0.83      0.63      0.71       235\n",
      "        Rock       0.91      0.97      0.94       966\n",
      "\n",
      "    accuracy                           0.90      1201\n",
      "   macro avg       0.87      0.80      0.83      1201\n",
      "weighted avg       0.90      0.90      0.90      1201\n",
      "\n",
      "\n",
      " Random Forest: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Hip-Hop       0.82      0.62      0.70       235\n",
      "        Rock       0.91      0.97      0.94       966\n",
      "\n",
      "    accuracy                           0.90      1201\n",
      "   macro avg       0.87      0.79      0.82      1201\n",
      "weighted avg       0.89      0.90      0.89      1201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Compare the performance of the model\n",
    "print(\"Logistic Regression: \\n\", classification_report(test_labels, CV_Tune_LogR.predict(test_pca)))\n",
    "print(\"\\n Decision Tree: \\n\", classification_report(test_labels, CV_Tuned_DT.predict(test_pca)))\n",
    "print(\"\\n Support Vector Machine: \\n\", classification_report(test_labels, CV_Tuned_SVM.predict(test_pca)))\n",
    "print(\"\\n K Nearest Neighbors: \\n\", classification_report(test_labels, CV_KNN.predict(test_pca)))\n",
    "print(\"\\n Random Forest: \\n\", classification_report(test_labels, CV_RF.predict(test_pca)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
